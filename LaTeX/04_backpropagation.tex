\newpage
\thispagestyle{empty}
\section{Backpropagation}\label{sec:backpropagation}   

\vspace{1cm}
\begin{tcolorbox}[title={Inhalt}]
Die Backpropagation umfasst folgende Elemente:
\begin{itemize}
\item Was ist eine geeignete Verlustfunktion?
\item Wie lernen Neuronale Netze?
\item Grundidee der Backpropagation
\item Gewichtsanpassung
\begin{quotation}
Eine Einleitung muss auch durch die Arbeit führen. Sie muss dem Leser helfen, sich in der Arbeit und ihrer Struktur zu Recht zu finden. Für jedes Kapitel sollte eine ganz kurze Inhaltsangabe gemacht werden und ggf. motiviert werden, warum es geschrieben wurde. Oft denkt sich ein Autor etwas bei der Struktur seiner Arbeit, auch solche Beweggründe sind dem Leser zu erklären\footnote{\cite{BBoJ}, S. 6}:. 
\end{quotation}
\end{itemize}
\end{tcolorbox}

\subsection{Was ist eine geeignete Verlustfunktion?}\label{subsec:backpropagation:verlustfunktion}
Die Verlustfunktion wird genutzt um den Fehler eines neuronalen Netzes zu berechnen. Dieser Fehler gibt an, 
wie sehr die wirklichen Ausgabewerte des neuronalen Netzes nach Eingabe eines Trainingsdatensatzes von den gewollten Ausgabewerten abweichen. 
\bigbreak\noindent
Eine weitverbreitete Verlustfunktion ist der sogenannte Mean Squared Error (MSE), bei dem der Durchschnitt der quadrierten Fehler berechnet wird.
Bei Anwendung auf neuronale Netze lässt sich diese Funktion wie folgt ausdrücken:

\begin{eqnarray}  
  C(w,b) = \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\end{eqnarray}

\noindent
In dieser Gleichung steht $w$ für die Gesamtheit aller Gewichte im neuronalen Netz, während $b$ alle Biases repräsentiert.
$x$ bezeichnet die Anzahl der Trainingsdatensätze und $y(x)$ ist der Vektor der Soll-Werte nach einem Trainingsbeispiel.
Schließlich ist $a$ der Vektor der tatsächlichen Ausgabewerte. 
\bigbreak\noindent
Wichtige Eigenschaften der MSE Verlustfunktion sind zum einen, dass sie niemals negativ wird, da jeder Term der Funktion positiv ist.
Außerdem sieht man recht einfach, dass $C(w, b) \approx 0$ gilt, wenn die die Soll-Werte ungefähr gleich den tatsächlichen Ausgabewerten sind.
\bigbreak\noindent
Es wird der durchschnittliche Fehler über \textbf{alle} Trainingsbeispiele berechnet. 
Es gibt Methoden, bei denen der Fehler nur für ausgewählte Teilgruppen (sog. Batches) von Trainingsbeispielen berechnet wird (Batch-Methode). 
Diese Vorgehensweise kann die Laufzeit des Backpropagation-Algorithmus verbessern, wird in diesem Text jedoch nicht weiter vertieft.
% TODO: Quelle angeben: http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent

\subsection{Wie lernen Neuronale Netze?}\label{subsec:backpropagation:lernen_nn}
Da wir nun eine Verlustfunktion kennen, welche den Fehler eines neuronalen Netzes bestimmt, kann man eben jene
Verlustfunktion nutzen um das neuronale Netz lernen zu lassen. Der Ausgabewert der Verlustfunktion hängt sowohl von allen Weights $w$ 
als auch von allen Biases $b$ des neuronalen Netzes ab. Nun sucht man die konkreten Werte für die Weights und Biases, damit die 
Verlustfunktion so klein wie möglich wird. Dies tut man mit dem bereits oben beschriebenen Gradientenverfahren.

\subsection{Grundidee Backpropagation}\label{subsec:backpropagation:grundiee}
\subsubsection{Fehlerrückführung}\label{subsec:backpropagation:fehlerrueckfuehrung}
  %\input{}
  TEXT FOLGT...

\subsubsection{Methode mit Matrizenmultiplikation}\label{subsec:backpropagation:matrizen}
  %\input{}
  TEXT FOLGT...

\subsubsection{Forward / Backward Phase erklären}\label{subsec:backpropagation:forward_backward}
  %\input{}
  TEXT FOLGT...

\subsection{Fehlerfunktion finden}\label{subsec:backpropagation:fehlerfunktion}

\subsection{Gewichtsanpassung (Maybe)}\label{subsec:backpropagation:gewichtsanpassung}
wie sollten initialisierungs-werte gewählt werden?
